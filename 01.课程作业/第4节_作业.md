# 基础作业
构建数据集，使用 XTuner 微调 InternLM-Chat-7B 模型, 让模型学习到它是你的智能小助手，效果如下图所示，本作业训练出来的模型的输出需要将不要葱姜蒜大佬替换成自己名字或昵称！
## 实操
### 1. 微调环境准备
- 激活环境  
```
conda create --name personal_assistant --clone=/root/share/conda_envs/internlm-base
conda activate personal_assistant
```
![激活环境](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/01.%E6%BF%80%E6%B4%BB%E7%8E%AF%E5%A2%83.png)  
- xtuner源码安装  
```
# 进入家目录 （~的意思是 “当前用户的home路径”）
cd ~
# 创建版本文件夹并进入，以跟随本教程
# personal_assistant用于存放本教程所使用的东西
mkdir /root/personal_assistant && cd /root/personal_assistant
mkdir /root/personal_assistant/xtuner019 && cd /root/personal_assistant/xtuner019
# 拉取 0.1.9 的版本源码
git clone -b v0.1.9  https://github.com/InternLM/xtuner
# 无法访问github的用户请从 gitee 拉取:
# git clone -b v0.1.9 https://gitee.com/Internlm/xtuner
# 进入源码目录
cd xtuner
# 从源码安装 XTuner
pip install -e '.[all]'
```  
![xtuner安装成功](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/02.xtuner.png)
### 2. 数据准备
- 创建data文件夹用于存放用于训练的数据集  
```
mkdir -p /root/personal_assistant/data && cd /root/personal_assistant/data
```  
- 在data目录下创建一个json文件personal_assistant.json作为本次微调所使用的数据集
- 在data目录下新建一个generate_data.py文件，用于生成数据集
- 生成数据准备  
![生成数据准备](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/03.%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87.png)
### 3. 配置准备
```
# 下载模型InternLM-chat-7B
mkdir -p /root/personal_assistant/model/Shanghai_AI_Laboratory
cp -r /root/share/temp/model_repos/internlm-chat-7b /root/personal_assistant/model/Shanghai_AI_Laboratory
# 列出所有内置配置
xtuner list-cfg
#创建用于存放配置的文件夹config并进入
mkdir /root/personal_assistant/config && cd /root/personal_assistant/config
# 拷贝一个配置文件到当前目录
xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .
```
- 模型下载  
![模型下载](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/04.%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.png)  
- 列出所有内置配置/拷贝  
![列出所有内置配置](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/04.%E9%85%8D%E7%BD%AE%E6%9F%A5%E7%9C%8B.png)  
![配置文件夹创建](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/05.%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%A4%B9%E5%88%9B%E5%BB%BA.png)
![内置配置拷贝](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/06.%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E6%8B%B7%E8%B4%9D.png)
### 4. 微调启动
- xtuner train命令启动训练  
```
xtuner train /root/personal_assistant/config/internlm_chat_7b_qlora_oasst1_e3_copy.py
```  
![微调启动](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/15.my_training.png)
### 5. 微调后参数转换/合并
- 训练后的pth格式参数转Hugging Face格式  
```
# 创建用于存放Hugging Face格式参数的hf文件夹
mkdir /root/personal_assistant/config/work_dirs/hf
export MKL_SERVICE_FORCE_INTEL=1
# 配置文件存放的位置
export CONFIG_NAME_OR_PATH=/root/personal_assistant/config/internlm_chat_7b_qlora_oasst1_e3_copy.py
# 模型训练后得到的pth格式参数存放的位置
export PTH=/root/personal_assistant/config/work_dirs/internlm_chat_7b_qlora_oasst1_e3_copy/epoch_3.pth
# pth文件转换为Hugging Face格式后参数存放的位置
export SAVE_PATH=/root/personal_assistant/config/work_dirs/hf
# 执行参数转换
xtuner convert pth_to_hf $CONFIG_NAME_OR_PATH $PTH $SAVE_PATH
```  
![ 微调后参数转换](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/16.path_to_df.png)  
- 微调后参数合并  
```
export MKL_SERVICE_FORCE_INTEL=1
export MKL_THREADING_LAYER='GNU'
# 原始模型参数存放的位置
export NAME_OR_PATH_TO_LLM=/root/personal_assistant/model/Shanghai_AI_Laboratory/internlm-chat-7b
# Hugging Face格式参数存放的位置
export NAME_OR_PATH_TO_ADAPTER=/root/personal_assistant/config/work_dirs/hf
# 最终Merge后的参数存放的位置
mkdir /root/personal_assistant/config/work_dirs/hf_merge
export SAVE_PATH=/root/personal_assistant/config/work_dirs/hf_merge
# 执行参数Merge
xtuner convert merge \
    $NAME_OR_PATH_TO_LLM \
    $NAME_OR_PATH_TO_ADAPTER \
    $SAVE_PATH \
    --max-shard-size 2GB
```  
![微调后参数合并 ](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/17.merge.png)
### 6. 网页DEMO
- 安装网页Demo所需依赖  
```
pip install streamlit==1.24.0
```  
![streamlit安装](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/13.streamlit.png)  
- 下载InternLM项目代码  
```
# 创建code文件夹用于存放InternLM项目代码
mkdir /root/personal_assistant/code && cd /root/personal_assistant/code
git clone https://github.com/InternLM/InternLM.git
```  
![下载InternLM项目代码 ](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/12.%E9%A1%B9%E7%9B%AE%E4%BB%A3%E7%A0%81.png)  
- 代码修改部分  
```
def load_model():
    model = (
        AutoModelForCausalLM.from_pretrained("/root/model/Shanghai_AI_Laboratory/internlm-chat-7b", trust_remote_code=True)
        .to(torch.bfloat16)
        .cuda()
    )
    tokenizer = AutoTokenizer.from_pretrained("/root/model/Shanghai_AI_Laboratory/internlm-chat-7b", trust_remote_code=True)
    return model, tokenizer
# 微调前加载的模型：/root/model/Shanghai_AI_Laboratory/internlm-chat-7b
# 微调后加载的模型：/root/personal_assistant/config/work_dirs/hf_merge
```  
- 运行 /root/personal_assistant/code/InternLM 目录下的 web_demo.py 文件  
```
streamlit run /root/personal_assistant/code/InternLM/web_demo.py --server.address 127.0.0.1 --server.port 6006
```  
注意：要在浏览器打开 http://127.0.0.1:6006 页面后，模型才会加载。在加载完模型之后，就可以与微调后的 InternLM-Chat-7B 进行对话  
![模型加载图](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/18.%E6%98%BE%E7%A4%BA%E7%AA%97%E5%8F%A3.png)

## 效果
### 微调前
- ```
  配置路径：/root/personal_assistant/config/work_dirs/hf_merge
  ```
- 微调前效果图  
![微调前效果图 ](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/18.%E5%BE%AE%E8%B0%83%E4%B9%8B%E5%89%8D.png)
### 微调后
- ```
  配置路径：/root/model/Shanghai_AI_Laboratory/internlm-chat-7b
  ```
- 微调后效果图
![微调后效果图](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/18.%E5%BE%AE%E8%B0%83%E4%B9%8B%E5%90%8E.png)
# 进阶作业
- 将训练好的Adapter模型权重上传到 OpenXLab、Hugging Face 或者 MoelScope 任一一平台。
- 将训练好后的模型应用部署到 OpenXLab 平台，参考部署文档请访问：https://aicarrier.feishu.cn/docx/MQH6dygcKolG37x0ekcc4oZhnCe
- -----在完成过程中---------
