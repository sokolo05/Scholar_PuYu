# LMDeploy 的量化和部署
## 基础作业：
使用 LMDeploy 以本地对话、网页Gradio、API服务中的一种方式部署 InternLM-Chat-7B 模型，生成 300 字的小故事（需截图）
### 1 环境配置  
- 首先使用 vgpu-smi  查看显卡资源使用情况![查看显卡资源使用](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/01.vgpu-smi.png)
- 基础环境配置
	```
	/root/share/install_conda_env_internlm_base.sh lmdeploy
	conda env list
	------------------------------------------------------------
	```  
	![基础环境配置](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/02.%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE.png)
- lmdeploy安装过慢问题 
	```
	# 手动安装flash_attn
	pip install /root/share/wheels/flash_attn-2.4.2+cu118torch2.0cxx11abiTRUE-cp310-cp310-linux_x86_64.whl
	# 手动安装lmdeploy
	```  
- 报错`bash: lmdeploy: command not found`  
    找不到下载的lmdeploy位置，要export告诉机器:一般情况下是下到了'/root/.local/bin',可留意下载之后的warning信息  
    `export PATH=$PATH:'/root/.local/bin'`
- lmdeploy安装成功截图  
	![lmdeploy安装成功](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/03.Imdeploy%E5%AE%89%E8%A3%85%E6%88%90%E5%8A%9F.png)
### 2 服务部署——>模型转换
- 使用 TurboMind 推理模型需要先将模型转化为 TurboMind 的格式，目前支持在线转换和离线转换两种形式。在线转换可以直接加载 Huggingface 模型，离线转换需需要先保存模型再加载  
- TurboMind 是一款关于 LLM 推理的高效推理引擎，基于英伟达的 FasterTransformer 研发而成。它的主要功能包括：LLaMa 结构模型的支持，persistent batch 推理模式和可扩展的 KV 缓存管理器
#### 1. 在线转换
lmdeploy 支持直接读取 Huggingface 模型权重，目前共支持三种类型:
- 在 huggingface.co 上面通过 lmdeploy 量化的模型，如 [llama2-70b-4bit](https://huggingface.co/lmdeploy/llama2-chat-70b-4bit), [internlm-chat-20b-4bit](https://huggingface.co/internlm/internlm-chat-20b-4bit)
- huggingface.co 上面其他 LM 模型，如 Qwen/Qwen-7B-Chat
- 在线转换示例:  
![在线转换](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/04.%E5%9C%A8%E7%BA%BF%E8%BD%AC%E6%8D%A2.png)
	```
	# 需要能访问 Huggingface 的网络环境
	
	# 加载使用 lmdeploy 量化的版本
	lmdeploy chat turbomind internlm/internlm-chat-20b-4bit --model-name internlm-chat-20b
	# 加载其他 LLM 模型
	lmdeploy chat turbomind Qwen/Qwen-7B-Chat --model-name qwen-7b
	
	# 直接启动本地的 Huggingface 模型
	lmdeploy chat turbomind /share/temp/model_repos/internlm-chat-7b/  --model-name internlm-chat-7b
	```  
#### 2. 离线转换
离线转换需要在启动服务之前，将模型转为 lmdeploy TurboMind 的格式  
```
# 转换模型（FastTransformer格式） TurboMind
lmdeploy convert internlm-chat-7b /path/to/internlm-chat-7b

# 官方提供的模型文件，保存在用户根目录执行
lmdeploy convert internlm-chat-7b  /root/share/temp/model_repos/internlm-chat-7b/
```  
执行完成后将会在当前目录生成一个 `workspace` 的文件夹。这里面包含的就是 `TurboMind` 和 `Triton` “模型推理”需要到的文件  
    ![workspace文件夹](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/05.workspace.png)  
`weights` 和 `tokenizer` 目录分别放的是拆分后的参数和 Tokenizer。如果我们进一步查看 weights 的目录，就会发现参数是按层和模块拆开的，如下图所示  
    ![weights目录](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/05.workspace_weights.png)

### 3 基础作业展示
#### 1. TurboMind 推理+命令行本地对话
模型转换完成后,具备使用模型推理的条件，可以进行真正的模型推理环节。先尝试本地对话（Bash Local Chat），下面用（Local Chat 表示）在这里其实是跳过 API Server 直接调用 TurboMind。简单来说，就是命令行代码直接执行 TurboMind  
执行命令如下：  
```
# Turbomind + Bash Local Chat
lmdeploy chat turbomind ./workspace
```  
启动后就可以和它进行对话了，如下图所示  
![启动对话](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/07.TurboMind%20%E6%8E%A8%E7%90%86%2B%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%9C%AC%E5%9C%B0%E5%AF%B9%E8%AF%9D-1.png)  
输入后两次回车，退出时输入exit 回车两次即可。此时，Server 就是本地跑起来的模型（TurboMind），命令行可以看作是前端，键入自己的问题写一个关于奥运会排球的300字小故事，如下图所示：  
![奥运故事](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/07.TurboMind%20%E6%8E%A8%E7%90%86%2B%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%9C%AC%E5%9C%B0%E5%AF%B9%E8%AF%9D-3.png)
#### 2. TurboMind推理+API服务
运用 lmdepoy 进行服务化——>”模型推理/服务“目前提供了 Turbomind 和 TritonServer 两种服务化方式。此时，Server 是 TurboMind 或 TritonServer，API Server 可以提供对外的 API 服务  
- 以下命令行代码启动服务： 
	```
	# ApiServer+Turbomind   api_server => AsyncEngine => TurboMind
	lmdeploy serve api_server ./workspace \
		--server_name 0.0.0.0 \
		--server_port 23333 \
		--instance_num 64 \
		--tp 1
	```  
	参数中 server_name 和 server_port 分别表示服务地址和端口，tp 参数我们之前已经提到过了，表示 Tensor 并行。还剩下一个 instance_num 参数，表示实例数，可以理解成 Batch 的大小，执行后如下图所示  
![启动服务](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/07.TurboMind%20%E6%8E%A8%E7%90%86%2BAPI-1.png)  
- 新开一个窗口，执行下面的 Client 命令。打开 vscode 的 Terminal，执行下面的命令：  
	```
	# ChatApiClient+ApiServer（注意是http协议，需要加http）
	lmdeploy serve api_client http://localhost:23333
	```  
* 新开窗口界面如下图：  
![新开一个窗口](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/07.TurboMind%20%E6%8E%A8%E7%90%86%2BAPI-2.png)
- API Server启动结果，借助接口打开 http://{host}:23333 查看，如下图所示：  
    ![API Server启动](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/08.fastAPI%E6%8E%A5%E5%8F%A3.png) 
    Server 在远程服务器，在本地需要做一下 ssh 转发才能直接访问  
    `ssh -CNg -L 23333:127.0.0.1:23333 root@ssh.intern-ai.org.cn -p 33383`
- 以 v1/chat/completions 接口为例，参数配置后，请求结果如下图：  
![请求结果1](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/09.Curl.png)  
![请求结果2](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/09.response.png)
#### 3. 网页Gradio的Demo 演示
 Gradio 需要本地访问展示界面，因此也需要通过 ssh 将数据转发到本地  
`ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p 33383`
1. TurboMind 服务作为后端，需要借助于启动的API Server  
API Server 的启动和上一节一样，这里直接启动作为前端的 Gradio  
![API Server的启动](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/10.%E7%BD%91%E9%A1%B5%20Demo%20%E6%BC%94%E7%A4%BA-1.png)
```
# Gradio+ApiServer。必须先开启 Server，此时 Gradio 为 Client
lmdeploy serve gradio http://0.0.0.0:23333 \
	--server_name 0.0.0.0 \
	--server_port 6006 \
	--restful_api True
```  
结果如下图所示  
![结果](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/10.%E7%BD%91%E9%A1%B5%20Demo%20%E6%BC%94%E7%A4%BA-2.png)  
2. TurboMind 推理作为后端  
Gradio 也可以直接和 TurboMind 连接  
```
# Gradio+Turbomind(local)
lmdeploy serve gradio ./workspace
```  
直接启动 Gradio，此时没有 API Server，TurboMind 直接与 Gradio 通信。如下图所示  
![结果](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/10.%E7%BD%91%E9%A1%B5%20Demo%20%E6%BC%94%E7%A4%BA-3.png)  
5. TurboMind 推理 + Python 代码集成
Python 直接与 TurboMind 进行交互
### 实践方案选择
#### step_1:  
“模型推理/服务”，推荐使用 TurboMind，使用简单，性能良好
#### step_2:  
API服务和Client分场景选择
1. 提供类似 OpenAI 那样的 HTTP 接口服务。推荐使用 TurboMind推理 + API 服务
2. 演示 Demo，Gradio 无疑是比 Local Chat 更友好的。推荐使用 TurboMind 推理作为后端的Gradio进行演示
3. 在自己的 Python 项目中使用大模型功能。推荐使用 TurboMind推理 + Python
4. 其他非 Python 项目中使用大模型功能。推荐直接通过 HTTP 接口调用服务。也就是用本列表第一条先启动一个 HTTP API 服务，然后在项目中直接调用接口
5.  C++项目，直接用 TurboMind 的 C++ 接口
## 进阶作业（可选做）

- 将第四节课训练自我认知小助手模型使用 LMDeploy 量化部署到 OpenXLab 平台。
- 对internlm-chat-7b模型进行量化，并同时使用KV Cache量化，使用量化后的模型完成API服务的部署，分别对比模型量化前后和 KV Cache 量化前后的显存大小（将 bs设置为 1 和 max len 设置为512）。  
- 在自己的任务数据集上任取若干条进行Benchmark测试，测试方向包括：  
（1）TurboMind推理+Python代码集成  
（2）在（1）的基础上采用W4A16量化  
（3）在（1）的基础上开启KV Cache量化  
（4）在（2）的基础上开启KV Cache量化  
（5）使用Huggingface推理
