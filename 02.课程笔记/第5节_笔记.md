# LMDeploy 大模型量化部署实践
## 基础知识学习
### 1 大规模部署背景
#### 模型部署
1. 定义
   1. 将训练好的模型在特定软硬件环境中启动的过程，使模型能够接收输入并返回预测结果
   2. 为了满足性能和效率的需求，常常需要对模型进行优化，例如模型压缩和硬件加速
2. 产品形态
   1. 云端、边缘计算端、移动端
3. 计算设备
   1. CPU、GPU、NPU、TPG等
#### 大模型特点
1. 内存开销巨大
   1. 庞大的参数量。 7B 模型仅权重就需要 14+G 内存
   2. 采用自回归生成 token，需要缓存 Attention 的 k/v,带来巨大的内存开销
2. 动态shape
   1. 请求数不固定
   2. Token逐个生成、且数量不定
3. 相对视觉模型，LLM结构简单
   1. Transformers 结构，大部分是 decoder-only  

![大模型部署-1](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E8%83%8C%E6%99%AF-1.png)
#### 大模型部署挑战
1. 设备
   1. 如何应对巨大的存储问题? 低存储设备 (消费级显卡、手机等)如何部署?
2. 推理
   1. 如何加速 token 的生成速度
   2. 如何解决动态shape，让推理可以不间断
   3. 如何有效管理和利用内存
3. 服务
   1. 如何提升系统整体吞吐量?
   2. 对于个体用户，如何降低响应时间?
#### 大模型部署方案
1. 技术点
   1. 模型并行
   2. 低比特量化
   3. transformer 计算和访存优化
   4. Continuous Batch
   5. Page Attention
2. 方案
   1. huggingface transformers
   2. 专门的推理加速框架
      1. 云端——> lmdeploy、vllm、tensorrt-Ilm、deepspeed、...
      2. 移动端——> llama.cpp、mlc-Illm、...  

![大模型部署-2](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E8%83%8C%E6%99%AF-2.png)
### 2 LMDeploy简介
![LMDeploy简介](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/LMDeploy%E7%AE%80%E4%BB%8B.png)  
#### 静态/动态推理性能比较
![静态/动态推理性能比较](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/%E6%8E%A8%E7%90%86%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83.png)  
#### 核心功能—量化
前提概念：
- 计算密集（compute-bound）: 指推理过程中，绝大部分时间消耗在数值计算上；针对计算密集型场景，可以通过使用更快的硬件计算单元来提升计算速
- 访存密集（memory-bound）: 指推理过程中，绝大部分时间消耗在数据读取上；针对访存密集型场景，一般通过减少访存次数、提高计算访存比或降低访存量来优化——>LLM 模型由于 Decoder Only 架构的特性，实际推理时大多数的时间都消耗在了逐 Token 生成阶段（Decoding 阶段），是典型的访存密集型场景
- LLM 模型推理中的访存密集问题解决：使用 KV Cache 量化和 4bit Weight Only 量化（W4A16）
  - KV Cache 量化 ——> 将逐 Token（Decoding）生成过程中的上下文 K 和 V 中间结果进行 INT8 量化（计算时再反量化），以降低生成过程中的显存占用
  - 4bit Weight 量化 ——> 将 FP16 的模型权重量化为 INT4，Kernel 计算时，访存量直接降为 FP16 模型的 1/4，大幅降低了访存成本
- Weight Only 是指仅量化权重，数值计算依然采用 FP16（需要将 INT4 权重反量化）  
1. 为什么要进行量化  
  量化是一种以参数或计算中间结果精度下降换空间节省（以及同时带来的性能提升）的策略，主要包括 KV Cache 量化和模型参数量化  
  ![为什么要进行量化](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/%E6%A0%B8%E5%BF%83%E9%87%8F%E5%8C%96-1.png)
  ![为什么做 Weight Only 的量化](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/%E6%A0%B8%E5%BF%83%E9%87%8F%E5%8C%96-2.png)
  ![如何做 Weight Only 的量化](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC5%E8%8A%82%E8%AF%BE/%E6%A0%B8%E5%BF%83%E9%87%8F%E5%8C%96-3.png)
2. KV Cache 量化（中间过程计算结果）
   KV Cache 量化是将已经生成序列的 KV 变成 Int8，使用过程一共包括三步:
  - 量化步骤
    - 计算 minmax。主要思路是通过计算给定输入样本在每一层不同位置处计算结果的统计情况
      - 对于 Attention 的 K 和 V：取每个 Head 各自维度在所有Token的最大、最小和绝对值最大值。对每一层来说，上面三组值都是 (num_heads, head_dim) 的矩阵。
      - 对于模型每层的输入：取对应维度的最大、最小、均值、绝对值最大和绝对值均值。每一层每个位置的输入都有对应的统计值，它们大多是 (hidden_dim, ) 的一维向量，当然在 FFN 层由于结构是先变宽后恢复，因此恢复的位置维度并不相同。 
    - 通过 minmax 获取量化参数。利用下面公式，获取每一层的 K V 中心值（zp）和缩放值（scale）  
      ```
      zp = (min+max) / 2
      scale = (max-min) / 255
      quant: q = round( (f-zp) / scale)
      dequant: f = q * scale + zp
      ```
    - 修改配置 —> 修改 weights/config.ini 文件，把 quant_policy 改为 4 
  - 量化效果 —> KV Cache 量化既能明显降低显存占用，还有可能同时带来精准度（Accuracy）的提升
3. W4A16 量化（模型参数）
  W4A16中的A是指Activation，保持FP16，只对参数进行 4bit 量化。使用过程也可以看作是三步：
  - 量化步骤
    - 计算 minmax。主要思路是通过计算给定输入样本在每一层不同位置处计算结果的统计情况
      - 对于 Attention 的 K 和 V：取每个 Head 各自维度在所有Token的最大、最小和绝对值最大值。对每一层来说，上面三组值都是 (num_heads, head_dim) 的矩阵
      - 对于模型每层的输入：取对应维度的最大、最小、均值、绝对值最大和绝对值均值。每一层每个位置的输入都有对应的统计值，它们大多是 (hidden_dim, ) 的一维向量，当然在 FFN 层由于结构是先变宽后恢复，因此恢复的位置维度并不相同
    - 量化权重模型。利用第一步得到的统计值对参数进行量化，具体又包括两小步：
      - 缩放参数
      - 整体量化
    - 转换成 TurboMind 格式
  - 量化效果 —> W4A16 参数量化后能极大地降低显存，同时相比其他框架推理速度具有明显优势
4. W4A16 量化模型和 KV Cache 量化可以一起使用，以达到最大限度节省显存
## 操作实现过程
第5节课作业实现过程：  
[第五节课作业](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E7%AC%AC5%E8%8A%82_%E4%BD%9C%E4%B8%9A.md)
## 其它知识学习
1. 服务部署和量化是没有直接关联的，量化的最主要目的是降低显存占用，主要包括两方面的显存：模型参数和中间过程计算结果。前者对应《3.2 W4A16 量化》，后者对应《3.1 KV Cache 量化》
    - 量化在降低显存的同时，一般还能带来性能的提升，因为更小精度的浮点数要比高精度的浮点数计算效率高，而整型要比浮点数高很多
    - 配置测试的步骤：
       ```
       Step1：优先尝试正常（非量化）版本，评估效果
        如果效果不行，需要尝试更大参数模型或者微调
        如果效果可以，跳到下一步
       Step2：尝试正常版本+KV Cache 量化，评估效果
        如果效果不行，回到上一步
        如果效果可以，跳到下一步
       Step3：尝试量化版本，评估效果
        如果效果不行，回到上一步
        如果效果可以，跳到下一步
       Step4：尝试量化版本+ KV Cache 量化，评估效果
        如果效果不行，回到上一步
        如果效果可以，使用方案
       ```
     - 量化版本选择——>考虑框架、显卡的支持情况
     - 实践经验设置：
        - 精度越高，显存占用越多，推理效率越低，但一般效果较好
        - Server 端推理一般用非量化版本或半精度、BF16、Int8 等精度的量化版本，比较少使用更低精度的量化版本
        - 端侧推理一般都使用量化版本，且大多是低精度的量化版本。这主要是因为计算资源所限  
  
     -  个人项目开发设置：
        - 如果资源足够（有GPU卡很重要），那就用非量化的正常版本
        - 如果没有 GPU 卡，只有 CPU（不管什么芯片），那还是尝试量化版本
        - 如果生成文本长度很长，显存不够，就开启 KV Cache
