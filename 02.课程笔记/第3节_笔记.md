# 基于 InternLM 和 LangChain 搭建你的知识库
## 前提介绍：
### LLM的局限性：
- 知识时效性受限
- 专业能力有限
- 定制化成本高
### 两种开发方式：
|RAG|Finetune|
|:----:|:----:|
|低成本|可个性化微调|
|可实时更新|知识覆盖面广|
|受基座模型影响大|成本高昂|
|单次回答知识有限|无法实时更新|
### RAG：检索增强生成基本思想框架
![检索增强生成基本思想框架](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/RAG%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6.png)
##  LangChain介绍
### 概念：
- LangChain 框架是一个开源工具，通过为各种 LLM 提供通用接口来简化应用程序的开发流程，帮助开发者自由构建 LLM应用
### 核心模块：
1. 链 (Chains) : 将组件组合实现端到端应用，通过一个对象封装实现一系列LLM 操作
2. Eg. 检索问答链，覆盖实现了 RAG (检索增强生成)的全部流程
### 基于LangChain搭建RAG应用
![RAG搭建](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/RAG%E6%90%AD%E5%BB%BA.png)
## 构建向量数据库
### 步骤：加载源文件 > 文档分块 > 文档向量化
1. 确定源文件类型，针对不同类型源文件选用不同的加载器
   - 核心在于将带格式文本转化为无格式字符串
2. 由于单个文档往往超过模型上下文上限，我们需要对加载的文档进行切分
   - 一般按字符串长度进行分割
   - 可以手动控制分割块的长度和重叠区间长度
3. 使用向量数据库来支持语义检索，需要将文档向量化存入向量数据库
   - 可以使用任一一种 Embedding 模型来进行向量化
   - 可以使用多种支持语义检索的向量数据库，一般使用轻量级的 Chroma
## 4. 搭建知识库助手
### 将InternLM接入LangChain
1. LangChain 支持自定义LLM，可以直接接入到框架中
2. 只将 InternLM 部署在本地，并封装一个自定义 LLM类，调用本地 InternLM 即可
### 构建检索问答链
1. LangChain作用：提供检索问答链模板，自动实现知识检索、Prompt嵌入、LLM问答的全部流程
2. 将基于 InternLM 的自定义 LLM 和已构建的向量数据库接入到检索问答链的上游
3. 调用检索问答链，即可实现知识库助手的核心功能
## RAG方案的不足及改进之处
### 性能核心受限之处：
* 检索精度
* Prompt性能
### 可优化之处
* 检索方面
  - 基于语义进行分割，保证每一个chunk的语义完整给
  - 给每一个chunk生成概括性索引，检索时匹配索引
* Prompt方面
  - 迭代优化Prompt策略
## 5. web Demo部署
- Eg：Gradio和Streamlit
# HuggingFace镜像使用
1. 使用huggingface 官方提供的 huggingface-cli 命令行工具
- (1) 安装依赖
   ```
   pip install -U huggingface_hub
-  (2) 基本命令示例
   ```
   export HF_ENDPOINT=https://hf-mirror.com
   huggingface-cli download --token hf_*** --resume-download --local-dir-use-symlinks False meta-llama/Llama-2-7b-hf --local-dir Llama-2-7b-hf
-  (3) 下载需要登录的模型（Gated Model）
   添加--token hf_***参数，其中hf_***是 access token，请在[huggingface官网](https://huggingface.co/settings/tokens)获取
   ```
   huggingface-cli download --token hf_*** --resume-download --local-dir-use-symlinks False meta-llama/Llama-2-7b-hf --local-dir Llama-2-7b-hf
2. 使用url直接下载时，将 huggingface.co 直接替换为本站域名hf-mirror.com。使用浏览器或者 wget -c、curl -L、aria2c 等命令行方式即可
3. (非侵入式，能解决大部分情况)huggingface 提供的包会获取系统变量，所以可以使用通过设置变量来解决
   ```
   HF_ENDPOINT=https://hf-mirror.com python your_script.py
# 基础作业——实操相关记录
## 1. 环境配置
1. InternLM 模型部署——>平台提供——>依赖包：modelscope==1.9.5、transformers==4.35.2、streamlit==1.24.0、sentencepiece==0.1.99、accelerate==0.24.1【过程简单-未截图】
2. 模型下载——>平台提供——> internlm-chat-7b【过程简单-未截图】
3.  LangChain 环境配置——> langchain==0.0.292、gradio==4.4.0、chromadb==0.4.15、sentence-transformers==2.2.2、unstructured==0.10.30、markdown==3.3.7、huggingface_hub
- huggingface下载文件![huggingface下载](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/00.HuggingFace_file.png)
5.  NLTK 资源——> punkt.zip、averaged_perceptron_tagger.zip
- NLTK相关文件 ![NLTK相关文件](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/00.nltk_data.png)
6.  项目所需代码——> [https://github.com/InternLM/tutorial](https://github.com/InternLM/tutorial)【过程简单-未截图】
## 2. 知识库搭建
1. 数据搜集：上海人工智能实验室开源的opencompass、lmdeploy、xtuner、InternLM-XComposer、lagent、InternLM仓库中的 markdown、txt 文件作为示例语料库
- 实现：函数递归指定文件夹路径，返回其中所有满足条件（即后缀名为 .md 或者 .txt 的文件）的文件路径
- 搜集文件的路径![文件路径示意图](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/001.file_list.png)
- 共有获取6个文件夹中的.md和.txt格式文件路径，部分文件路径截图 ![部分文件路径截图](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/002.file_list.png)
2. 加载数据：LangChain.FileLoader 对象来加载目标文件，得到由目标文件解析出的纯文本内容
- 加载后得到docs格式的纯文本对象
- Docs格式文件部分截图 ![Docs格式文件部分截图](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/003.docs.png)
3. 构建向量数据库：引入到 LangChain 框架中构建向量数据库。由纯文本对象构建向量数据库，先对文本进行分块(langchain.text_splitter.RecursiveCharacterTextSplitter)——>接着对文本块进行向量化开源词向量模型[Sentence Transformer](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)(langchain.embeddings.huggingface.HuggingFaceEmbeddings)——>语料加载到指定路径下的向量数据库,持久化到磁盘上(from langchain.vectorstores.Chroma)
- text_splitter和split_docs部分数据截图 ![text_splitter和split_docs部分数据截图](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/004.text_splitter.png)
- embeddings及vectordb展示 ![embeddings及vectordb展示](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/005.embedding.png)
## 3. InternLM 接入 LangChain
1. 从 LangChain.llms.base.LLM 类继承一个子类，并重写构造函数与 _call 函数。确保完全一致的方式调用 LangChain 的接口，而无需考虑底层模型调用的不一致【过程简单-未截图】
## 4. 构建检索问答链
1. 加载向量数据库
- 将上文构建的向量数据库导入进来，我们可以直接通过 Chroma 以及上文定义的词向量模型来加载已构建的数据库——>生成的对象可以针对用户的 query 进行语义向量检索，得到与用户提问相关的知识片段
- embeddings、加载数据库vectordb ![embeddings、加载数据库vectordb](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/005.embedding.png)
2. 实例化自定义 LLM 与 Prompt Template
- 构建的Prompt Template基于一个带变量的字符串，在检索之后，LangChain 会将检索到的相关文档片段填入到 Template 的变量中，从而实现带知识的 Prompt 构建
- 实例化LLM 与 Prompt Template ![实例化LLM 与 Prompt Template](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/006.instance.png)
3. 构建检索问答链
- 调用 LangChain 提供的检索问答链构造函数，基于我们的自定义 LLM、Prompt Template 和向量知识库来构建一个基于 InternLM 的检索问答链
- 检索问答链结果 ![检索问答链结果](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/006.result.png)
## 5. 部署 Web Demo
- 基于 Gradio 框架将其部署到 Web 网页，搭建一个小型 Demo
- 网页Demo样例图 ![网页Demo样例图](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/05.InternLM.png)
- 问题：毛泽东是谁 ——>执行结果图 ![执行结果图](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/05.InternLM_1.png)
