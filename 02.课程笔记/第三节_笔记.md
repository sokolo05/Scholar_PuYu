# 基于 InternLM 和 LangChain 搭建你的知识库
## 前提介绍：
### LLM的局限性：
- 知识时效性受限
- 专业能力有限
- 定制化成本高
### 两种开发方式：
|RAG|Finetune|
|:----:|:----:|
|低成本|可个性化微调|
|可实时更新|知识覆盖面广|
|受基座模型影响大|成本高昂|
|单次回答知识有限|无法实时更新|
### RAG：检索增强生成基本思想框架
![检索增强生成基本思想框架](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/RAG%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6.png)
##  LangChain介绍
### 概念：
- LangChain 框架是一个开源工具，通过为各种 LLM 提供通用接口来简化应用程序的开发流程，帮助开发者自由构建 LLM应用
### 核心模块：
1. 链 (Chains) : 将组件组合实现端到端应用，通过一个对象封装实现一系列LLM 操作
2. Eg. 检索问答链，覆盖实现了 RAG (检索增强生成)的全部流程
### 基于LangChain搭建RAG应用
![RAG搭建](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/RAG%E6%90%AD%E5%BB%BA.png)
## 构建向量数据库
### 步骤：加载源文件 > 文档分块 > 文档向量化
1. 确定源文件类型，针对不同类型源文件选用不同的加载器
   - 核心在于将带格式文本转化为无格式字符串
2. 由于单个文档往往超过模型上下文上限，我们需要对加载的文档进行切分
   - 一般按字符串长度进行分割
   - 可以手动控制分割块的长度和重叠区间长度
3. 使用向量数据库来支持语义检索，需要将文档向量化存入向量数据库
   - 可以使用任一一种 Embedding 模型来进行向量化
   - 可以使用多种支持语义检索的向量数据库，一般使用轻量级的 Chroma
## 4. 搭建知识库助手
### 将InternLM接入LangChain
1. LangChain 支持自定义LLM，可以直接接入到框架中
2. 只将 InternLM 部署在本地，并封装一个自定义 LLM类，调用本地 InternLM 即可
### 构建检索问答链
1. LangChain作用：提供检索问答链模板，自动实现知识检索、Prompt嵌入、LLM问答的全部流程
2. 将基于 InternLM 的自定义 LLM 和已构建的向量数据库接入到检索问答链的上游
3. 调用检索问答链，即可实现知识库助手的核心功能
## RAG方案的不足及改进之处
### 性能核心受限之处：
* 检索精度
* Prompt性能
### 可优化之处
* 检索方面
  - 基于语义进行分割，保证每一个chunk的语义完整给
  - 给每一个chunk生成概括性索引，检索时匹配索引
* Prompt方面
  - 迭代优化Prompt策略
## 5. web Demo部署
- Eg：Gradio和Streamlit
