# XTuner 大模型单卡低成本微调实战
# 理论知识点汇总
## 1. Finetune 简介
大模型在海量数据上通过半监督或者无监督的方式训练，在具体的LLM 的下游应用中，增量预训练和指令跟随是经常会用到两种的微调模式
 ![两种策略](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/01.%E4%B8%A4%E7%A7%8D%E7%AD%96%E7%95%A5.png)
 ![微调](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/03.%E5%BE%AE%E8%B0%83.png)
### 1. 增量预训练微调
- 使用场景:让基座模型学习到一些新知识，如某个垂类领域的常识训练数据:文章、书籍、代码等
- 增量预训练微调原理
![增量预训练微调原理](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/06.%E5%A2%9E%E9%87%8F%E5%BE%AE%E8%B0%83%E5%8E%9F%E7%90%86.png)
### 2. 指令跟随微调
- 使用场景: 让模型学会对话模板，根据人类指令进行对话训练数据: 高质量的对话、问答数据
- 指令跟随微调是为了得到能够实际对话的 LLM介绍指令跟随微调前，需要先了解如何使用 LLM 进行对话
![指令跟随微调](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/02.%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83.png)
- 在实际对话时，通常会有三种角色，对话过程中三种角色不会被感知
1. System  给定一些上下文信息，比如“你是一个安全的 AI 助手”
2. User  实际用户，会提出一些问题，比如“世界第一高峰是?”
3. Assistant  根据 User 的输入，结合 System 的上下文信息，做出回答，比如“珠穆朗玛峰”
- 两种对话模板
![两种对话模板](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/04.%E4%B8%A4%E7%A7%8D%E5%AF%B9%E8%AF%9D%E6%A8%A1%E6%9D%BF.png)
- 指令跟随微调原理
![指令跟随微调原理](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/05.%E6%8C%87%E4%BB%A4%E5%BE%AE%E8%B0%83%E5%8E%9F%E7%90%86.png)
### 3. XTuner微调（LORA & QLORA）
场景描述：想象一下，你有一个超大的玩具，现在你想改造这个超大的玩具。但是，对整个玩具进行全面的改动会非常昂贵
1. LoRA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS
   - [LoRA](https://arxiv.org/abs/2106.09685) 方法：只对玩具中的某些零件进行改动，而不是对整个玩具进行全面改动
   - LLM 的参数量主要集中在模型中的 Linear，训练这些参数会耗费大量的显存
   - LORA 通过在原本的 Linear 旁，新增一个支路，包含两个连续的小 Linear,新增的这个支路通常叫做 Adapter
   - Adapter 参数量远小于原本的 Linear，能大幅降低训练的显存消耗
2. [QLoRA](https://arxiv.org/abs/2305.14314) 方法：是 LoRA 的一种改进：如果你手里只有一把生锈的螺丝刀，也能改造你的玩具
## 2. XTuner 介绍
### 功能亮点
#### 1. 适配多种生态
1. 多种微调算法——>多种微调策略与算法，覆盖各类 SFT 场景
2. 适配多种开源生态——>支持加载 HuggingFace、ModelScope 模型或数据集
3. 自动优化加速——>开发者无需关注复杂的显存优化与计算加速细节
![优化与计算加速](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/07.%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%E5%99%A8%E8%B0%83%E5%BA%A6.png)
#### 2. 适配多种硬件
1. 训练方案覆盖 NVIDIA 20 系以上所有显卡
2. 最低只需 8GB 显存即可微调 7B 模型
#### 3. XTuner 技术架构图
![XTuner 技术架构图](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/08.XTuner%20%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84%E5%9B%BE.png)
## 3. 8GB 显卡玩转 LLM
### XTuner最重要的两个优化技巧:Flash Attention 和 DeepSpeed ZeRO 
#### 1. Flash Attention
1. Flash Attention 将 Attention 计算并行化避免了计算过程中 Attention Score NxN的显存占用 (训练过程中的 N 都比较大)
2. Flash Attention原理架构图
- ![Flash Attention原理架构图](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/09.Flash%20Attention.png)
#### 2. DeepSpeed ZeRO 
1. ZeRO 优化，通过将训练过程中的参数、梯度和优化器状态切片保存，能够在多 GPU 训练时显著节省显存
2. 除了将训练中间状态切片外，DeepSpeed 训练时使用FP16的权重，相较于 Pytorch 的AMP 训练在单 GPU 上也能大幅节省显存
3. DeepSpeed ZeRO原理架构图 ![DeepSpeed ZeRO原理架构图](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/10.DeepSpeed%20ZeRO.png)
#### 3. 两种策略存在问题及改进
1. 存在问题：DeepSpeed 和 Flash Attention 虽然能够大幅降低训练成本，但使用门槛相对较高，需要复杂的配置，甚至修改代码
2. 改进之处：为了让开发者专注于数据，XTuner会自动dispatch Flash Attention，并一键启动DeepSpeed ZeRO
#### 4. 优化前后性能比对
1. 优化前后性能比对 ![优化前后性能比对](https://github.com/sokolo05/Scholar_PuYu/blob/main/01.%E8%AF%BE%E7%A8%8B%E4%BD%9C%E4%B8%9A/%E5%9B%BE%E7%89%87/%E7%AC%AC4%E8%8A%82%E8%AF%BE/11.%E4%BC%98%E5%8C%96%E5%89%8D%E5%90%8E%E5%AF%B9%E6%AF%94.png)
## 4. 动手实战环节



## 5. 其他学习
### tmux 命令学习
1. 简介：tmux 是一个终端复用工具，它允许您在单个终端窗口中同时运行多个终端会话。使用 tmux 可以帮助您更高效地管理多个任务，避免打开过多的终端窗口
2. 常用的 tmux 命令及介绍
```
- tmux new：创建一个新的 tmux 会话
- tmux attach：连接到最近的 tmux 会话
- tmux list-sessions：列出当前存在的 tmux 会话
- tmux switch -t <session-name>：切换到指定的 tmux 会话
- tmux detach：从当前 tmux 会话中断开连接
```
3. 示例命令
  ```
# 创建一个新的 tmux 会话
tmux new -s tune
# 在 tmux 会话中运行一个命令
xtuner train ./internlm_chat_7b_qlora_oasst1_e3_copy.py --deepspeed deepspeed_zero2
# 按下 Ctrl+b d 断开当前 tmux 会话
# 下次需要连接到该会话时，执行以下命令：
tmux attach -t tune
```
### xtuner chat 的启动参数
| 启动参数              | 干哈滴                                                       |
| --------------------- | ------------------------------------------------------------ |
| **--prompt-template** | 指定对话模板                                                 |
| --system              | 指定SYSTEM文本                                               |
| --system-template     | 指定SYSTEM模板                                               |
| -**-bits**            | LLM位数                                                      |
| --bot-name            | bot名称                                                      |
| --with-plugins        | 指定要使用的插件                                             |
| **--no-streamer**     | 是否启用流式传输                                             |
| **--lagent**          | 是否使用lagent                                               |
| --command-stop-word   | 命令停止词                                                   |
| --answer-stop-word    | 回答停止词                                                   |
| --offload-folder      | 存放模型权重的文件夹（或者已经卸载模型权重的文件夹）         |
| --max-new-tokens      | 生成文本中允许的最大 `token` 数量                                |
| **--temperature**     | 温度值(模型活跃度)                                                      |
| --top-k               | 保留用于顶k筛选的最高概率词汇标记数                          |
| --top-p               | 如果设置为小于1的浮点数，仅保留概率相加高于 `top_p` 的最小一组最有可能的标记 |
| --seed                | 用于可重现文本生成的随机种子                                 |
